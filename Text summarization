import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import defaultdict

# Download required NLTK data (run once)
nltk.download('punkt')
nltk.download('stopwords')

def summarize_text(text, summary_ratio=0.3):
    sentences = sent_tokenize(text)
    words = word_tokenize(text.lower())

    stop_words = set(stopwords.words("english"))

    word_freq = defaultdict(int)
    for word in words:
        if word.isalnum() and word not in stop_words:
            word_freq[word] += 1

    sentence_score = defaultdict(int)
    for sentence in sentences:
        for word in word_tokenize(sentence.lower()):
            if word in word_freq:
                sentence_score[sentence] += word_freq[word]

    summary_length = int(len(sentences) * summary_ratio)
    summary_sentences = sorted(sentence_score, key=sentence_score.get, reverse=True)[:summary_length]

    summary = " ".join(summary_sentences)
    return summary


# ----------- Example Usage -----------
if __name__ == "__main__":
    text = """
    Artificial Intelligence is one of the most rapidly growing technologies.
    It is widely used in healthcare, education, finance, and transportation.
    AI systems can analyze large amounts of data efficiently.
    Machine learning and deep learning are subsets of AI.
    However, ethical concerns and job displacement are challenges of AI.
    """

    print("ORIGINAL TEXT:\n")
    print(text)

    print("\nSUMMARY:\n")
    print(summarize_text(text))
